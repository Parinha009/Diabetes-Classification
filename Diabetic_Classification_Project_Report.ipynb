{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b3d6bc",
   "metadata": {},
   "source": [
    "# Diabetes Classification Project\n",
    "\n",
    "**Author**: Parinha  \n",
    "**Course**: Machine Learning with Python\n",
    "\n",
    "## 1. Brief Summary of the Project\n",
    "\n",
    "This project addresses the challenge of early diabetes detection through machine learning techniques. Using the Pima Indians Diabetes dataset, we develop predictive models to identify patients at risk of diabetes based on various medical indicators. The primary goal is to create an accurate classification model that can assist medical professionals in diagnosing diabetes at an early stage. This has significant potential for improving healthcare outcomes by enabling timely intervention and treatment strategies for at-risk patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe29380",
   "metadata": {},
   "source": [
    "## 2. Healthcare Impact of the Project\n",
    "\n",
    "This project has significant implications for healthcare:\n",
    "\n",
    "- **Early Detection**: By identifying high-risk individuals before symptoms become severe, interventions can be initiated earlier.\n",
    "  \n",
    "- **Resource Optimization**: Healthcare systems can prioritize resources for patients with higher predicted risk.\n",
    "  \n",
    "- **Personalized Medicine**: The risk factors identified can help tailor treatment approaches to individual patient profiles.\n",
    "  \n",
    "- **Public Health Planning**: Insights from the model can inform broader public health strategies for diabetes prevention.\n",
    "  \n",
    "- **Educational Tool**: The visualizations and analysis serve as educational resources for understanding diabetes risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad717ea",
   "metadata": {},
   "source": [
    "## 3. Research Questions\n",
    "\n",
    "The project seeks to answer the following research questions:\n",
    "\n",
    "1. Which demographic and health indicators are most strongly associated with diabetes risk?\n",
    "2. How accurately can machine learning models predict diabetes based on these indicators?\n",
    "3. Which machine learning approach provides the best performance for diabetes classification?\n",
    "4. What threshold of these indicators significantly increases the risk of diabetes?\n",
    "5. How can these insights be translated into practical screening recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4826d",
   "metadata": {},
   "source": [
    "## 4. Approach to Answering the Research Questions\n",
    "\n",
    "My approach involved a comprehensive machine learning pipeline:\n",
    "\n",
    "1. **Data Understanding**: Exploring the Pima Indians Diabetes dataset to understand its structure and characteristics.\n",
    "2. **Data Preprocessing**: Handling missing values, standardizing features, and preparing the data for modeling.\n",
    "3. **Exploratory Data Analysis**: Visualizing relationships between features and the target variable.\n",
    "4. **Model Training**: Implementing multiple machine learning models to compare their performance.\n",
    "5. **Model Evaluation**: Assessing models using various metrics relevant to healthcare applications.\n",
    "6. **Feature Importance Analysis**: Identifying which health indicators contribute most to diabetes risk.\n",
    "7. **Model Selection**: Choosing the best performing model based on appropriate evaluation metrics.\n",
    "8. **Deployment**: Creating a reusable pipeline for making predictions on new patient data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d4802",
   "metadata": {},
   "source": [
    "## 5. Individual Contribution to the Project\n",
    "\n",
    "My individual contributions to this project include:\n",
    "\n",
    "- Implementing a comprehensive data preprocessing pipeline to handle missing and zero values in medical data.\n",
    "- Creating informative data visualizations to explore relationships between health indicators and diabetes.\n",
    "- Training and evaluating multiple machine learning models for diabetes classification.\n",
    "- Developing a thorough model evaluation framework focused on healthcare-relevant metrics.\n",
    "- Building a prediction system that can be used on new patient data.\n",
    "- Documenting insights and findings that can inform medical decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74670d81",
   "metadata": {},
   "source": [
    "## 6. Details about the Dataset Used, Including Visualizations\n",
    "\n",
    "The Pima Indians Diabetes dataset is a collection of medical data from 768 female patients of Pima Indian heritage, aged 21 years and older. It contains various health metrics and a binary outcome variable indicating whether the patient developed diabetes within five years.\n",
    "\n",
    "### Dataset Columns:\n",
    "- **Pregnancies**: Number of pregnancies the patient has had\n",
    "- **Glucose**: Plasma glucose concentration (mg/dL)\n",
    "- **BloodPressure**: Diastolic blood pressure (mm Hg)\n",
    "- **SkinThickness**: Triceps skinfold thickness (mm)\n",
    "- **Insulin**: 2-Hour serum insulin (mu U/ml)\n",
    "- **BMI**: Body Mass Index (weight in kg/(height in m)Â²)\n",
    "- **DiabetesPedigreeFunction**: Diabetes pedigree function (a function of diabetes history in relatives)\n",
    "- **Age**: Age in years\n",
    "- **Outcome**: Binary variable (1: has diabetes, 0: no diabetes)\n",
    "\n",
    "### Data Preparation:\n",
    "The dataset required careful preprocessing due to the presence of zero values in features that biologically cannot be zero (like Glucose or BMI). These were treated as missing data and replaced with median values.\n",
    "\n",
    "Let's load our dataset and explore its characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e17ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid') if 'seaborn-v0_8-whitegrid' in plt.style.available else plt.style.use('default')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Basic statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e81427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check for zero values in features that biologically shouldn't be zero\n",
    "features_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for feature in features_with_zeros:\n",
    "    zero_count = (data[feature] == 0).sum()\n",
    "    if zero_count > 0:\n",
    "        print(f\"Number of zeros in {feature}: {zero_count} ({zero_count/len(data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d62e94",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Let's create some visualizations to better understand the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "outcome_counts = data['Outcome'].value_counts()\n",
    "ax = sns.barplot(x=outcome_counts.index, y=outcome_counts.values, palette='viridis')\n",
    "plt.title('Distribution of Diabetes Outcome')\n",
    "plt.xlabel('Outcome (0: Non-diabetic, 1: Diabetic)')\n",
    "plt.ylabel('Count')\n",
    "for i, v in enumerate(outcome_counts.values):\n",
    "    plt.text(i, v + 5, str(v), ha='center')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage\n",
    "diabetic_percentage = outcome_counts[1] / outcome_counts.sum() * 100\n",
    "print(f\"Percentage of diabetic patients: {diabetic_percentage:.2f}%\")\n",
    "print(f\"Percentage of non-diabetic patients: {100 - diabetic_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e681c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by outcome\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(data.columns[:-1]):  # Exclude 'Outcome'\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(data=data, x=feature, hue='Outcome', kde=True, palette='viridis')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf638c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data.corr()\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', mask=mask)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()\n",
    "\n",
    "# Print the features most correlated with outcome\n",
    "corr_with_outcome = correlation_matrix['Outcome'].sort_values(ascending=False)\n",
    "print(\"Features most correlated with diabetes outcome:\")\n",
    "print(corr_with_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairplot of the most important features\n",
    "# Select the most important features based on correlation with outcome\n",
    "top_features = corr_with_outcome.index[1:5]  # Skip 'Outcome' itself\n",
    "selected_data = data[list(top_features) + ['Outcome']]\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(selected_data, hue='Outcome', palette='viridis')\n",
    "plt.suptitle('Pairplot of Top Correlated Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e843e",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Model Chosen and Justification\n",
    "\n",
    "This project is a binary classification task as we aim to predict a binary outcome: whether a patient has diabetes (1) or not (0). \n",
    "\n",
    "After evaluating multiple models, **Random Forest** was selected as the final model for implementation based on its superior overall performance.\n",
    "\n",
    "**Justification for Choice:**\n",
    "1. **Balanced Performance**: Random Forest provides excellent balance between precision and recall, which is crucial in a healthcare context where both false positives and false negatives have significant implications.\n",
    "2. **Feature Importance**: It provides valuable insights into feature importance, helping identify the most significant indicators for diabetes risk.\n",
    "3. **Robustness to Overfitting**: As an ensemble method, Random Forest is less prone to overfitting compared to individual decision trees.\n",
    "4. **Non-linearity**: It captures complex non-linear relationships between features that might be missed by simpler models like Logistic Regression.\n",
    "5. **Minimal Hyperparameter Tuning**: Even with default parameters, Random Forest performs well, making it a practical choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9014901",
   "metadata": {},
   "source": [
    "## 8. Alternative Models Explored and Comparison\n",
    "\n",
    "In addition to Random Forest, the following alternative models were implemented and evaluated:\n",
    "\n",
    "1. **Logistic Regression**: A linear model that is simple, interpretable, and serves as a good baseline.\n",
    "2. **Decision Tree**: A non-linear model that creates clear decision rules.\n",
    "3. **Support Vector Machine (SVM)**: A powerful model capable of finding complex decision boundaries.\n",
    "\n",
    "Let's implement and compare these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37880c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Handle zero values that should be considered as missing\n",
    "features_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "processed_data = data.copy()\n",
    "\n",
    "for feature in features_with_zeros:\n",
    "    # Replace zeros with NaN and fill with median\n",
    "    processed_data[feature] = processed_data[feature].replace(0, np.nan)\n",
    "    median_value = processed_data[feature].median()\n",
    "    processed_data[feature].fillna(median_value, inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = processed_data.drop('Outcome', axis=1)\n",
    "y = processed_data['Outcome']\n",
    "\n",
    "# Import necessary libraries for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(probability=True)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "results_df\n",
    "\n",
    "# Visualize model performance\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.barplot(x=results_df.index, y=results_df[metric], palette='viridis')\n",
    "    plt.title(f'Model Comparison: {metric.capitalize()}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1.0)\n",
    "    for j, v in enumerate(results_df[metric]):\n",
    "        plt.text(j, v + 0.02, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the best model based on F1 score (balancing precision and recall)\n",
    "best_model_name = results_df['f1'].idxmax()\n",
    "print(f\"\\nBest model based on F1 score: {best_model_name}\")\n",
    "print(f\"F1 Score: {results_df.loc[best_model_name, 'f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7c47d",
   "metadata": {},
   "source": [
    "## 9. Evaluation Techniques Used\n",
    "\n",
    "To ensure the model's reliability and accuracy, I employed the following evaluation techniques:\n",
    "\n",
    "### Train-Test Split\n",
    "The dataset was divided into:\n",
    "- Training set (80%): Used to train the models\n",
    "- Testing set (20%): Used to evaluate model performance on unseen data\n",
    "\n",
    "### Evaluation Metrics\n",
    "Multiple metrics were used to provide a comprehensive assessment:\n",
    "\n",
    "1. **Accuracy**: Overall correctness of predictions (correct predictions / total predictions)\n",
    "2. **Precision**: Ability to avoid false positives (true positives / (true positives + false positives))\n",
    "3. **Recall**: Ability to find all positive cases (true positives / (true positives + false negatives))\n",
    "4. **F1 Score**: Harmonic mean of precision and recall, providing balance between the two\n",
    "5. **ROC-AUC**: Area under the Receiver Operating Characteristic curve, measuring the model's ability to discriminate between classes\n",
    "\n",
    "### Confusion Matrix\n",
    "This visualization provides a detailed breakdown of correct and incorrect classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Create and display confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "\n",
    "# Display classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503419e",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "To optimize the performance of our selected Random Forest model, I performed hyperparameter tuning using GridSearchCV, which systematically searches through a predefined set of hyperparameter values to find the optimal combination.\n",
    "\n",
    "The hyperparameters tuned include:\n",
    "- Number of estimators (trees)\n",
    "- Maximum depth of each tree\n",
    "- Minimum samples required to split a node\n",
    "- Minimum samples required at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"Performing hyperparameter tuning for Random Forest...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest F1 Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_tuned = best_tuned_model.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_precision = precision_score(y_test, y_pred_tuned)\n",
    "tuned_recall = recall_score(y_test, y_pred_tuned)\n",
    "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "tuned_roc_auc = roc_auc_score(y_test, best_tuned_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"\\nPerformance of Tuned Random Forest on Test Set:\")\n",
    "print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"Precision: {tuned_precision:.4f}\")\n",
    "print(f\"Recall: {tuned_recall:.4f}\")\n",
    "print(f\"F1 Score: {tuned_f1:.4f}\")\n",
    "print(f\"ROC-AUC: {tuned_roc_auc:.4f}\")\n",
    "\n",
    "# Compare with untuned model\n",
    "print(\"\\nComparison with Untuned Random Forest:\")\n",
    "print(f\"F1 Score Improvement: {tuned_f1 - results['Random Forest']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870c7ad",
   "metadata": {},
   "source": [
    "## 11. Accuracy/Performance of the Model\n",
    "\n",
    "The tuned Random Forest model achieved excellent performance metrics:\n",
    "\n",
    "- **Accuracy**: Percentage of correctly classified instances\n",
    "- **Precision**: Ability to correctly identify diabetic patients (minimizing false alarms)\n",
    "- **Recall**: Ability to find all diabetic patients (minimizing missed cases)\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under the ROC curve, with higher values indicating better discrimination\n",
    "\n",
    "Let's visualize the feature importance to understand which health indicators contribute most to the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the tuned Random Forest model\n",
    "feature_importances = best_tuned_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "sorted_features = [features[i] for i in sorted_idx]\n",
    "sorted_importances = feature_importances[sorted_idx]\n",
    "\n",
    "# Create bar plot of feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=sorted_importances, y=sorted_features, palette='viridis')\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(sorted_features, sorted_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd3138",
   "metadata": {},
   "source": [
    "## 12. Assessment of Underfitting and Overfitting\n",
    "\n",
    "To ensure our model is neither underfitting nor overfitting, I evaluated its performance on both the training and test datasets:\n",
    "\n",
    "- **Underfitting**: Occurs when the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test sets.\n",
    "- **Overfitting**: Happens when the model learns the training data too well, including its noise, leading to excellent training performance but poor generalization to new data.\n",
    "\n",
    "Let's examine how our model performs on both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ba3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess overfitting/underfitting by comparing training and test performance\n",
    "# Get predictions on training set\n",
    "y_train_pred = best_tuned_model.predict(X_train)\n",
    "y_test_pred = best_tuned_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for both sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "train_roc_auc = roc_auc_score(y_train, best_tuned_model.predict_proba(X_train)[:, 1])\n",
    "test_roc_auc = roc_auc_score(y_test, best_tuned_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Compare training vs test performance\n",
    "print(\"Training vs. Test Performance:\")\n",
    "print(f\"Accuracy - Training: {train_accuracy:.4f}, Test: {test_accuracy:.4f}, Difference: {train_accuracy - test_accuracy:.4f}\")\n",
    "print(f\"F1 Score - Training: {train_f1:.4f}, Test: {test_f1:.4f}, Difference: {train_f1 - test_f1:.4f}\")\n",
    "print(f\"ROC-AUC - Training: {train_roc_auc:.4f}, Test: {test_roc_auc:.4f}, Difference: {train_roc_auc - test_roc_auc:.4f}\")\n",
    "\n",
    "# Visualize the comparison\n",
    "metrics = ['Accuracy', 'F1 Score', 'ROC-AUC']\n",
    "training_scores = [train_accuracy, train_f1, train_roc_auc]\n",
    "test_scores = [test_accuracy, test_f1, test_roc_auc]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, training_scores, width, label='Training Set')\n",
    "plt.bar(x + width/2, test_scores, width, label='Test Set')\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Training vs. Test Performance')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(training_scores):\n",
    "    plt.text(i - width/2, v + 0.02, f'{v:.3f}', ha='center')\n",
    "for i, v in enumerate(test_scores):\n",
    "    plt.text(i + width/2, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Provide interpretation\n",
    "diff = [train_accuracy - test_accuracy, train_f1 - test_f1, train_roc_auc - test_roc_auc]\n",
    "avg_diff = np.mean(diff)\n",
    "\n",
    "if avg_diff > 0.1:\n",
    "    print(\"\\nInterpretation: The model shows signs of overfitting as performance on the training set is significantly better than on the test set.\")\n",
    "elif avg_diff < 0.03:\n",
    "    print(\"\\nInterpretation: The model shows good generalization with minimal difference between training and test performance.\")\n",
    "else:\n",
    "    print(\"\\nInterpretation: The model shows some gap between training and test performance, but it's within an acceptable range for this application.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d074b",
   "metadata": {},
   "source": [
    "## 13. Key Learnings from the Project\n",
    "\n",
    "This project yielded several important insights:\n",
    "\n",
    "1. **Feature Importance**: Glucose levels, BMI, and Age emerged as the most significant predictors of diabetes risk, which aligns with medical knowledge about risk factors.\n",
    "\n",
    "2. **Data Quality Challenges**: Missing data indicated as zeros in medical datasets required careful preprocessing to avoid biasing the model.\n",
    "\n",
    "3. **Model Selection Considerations**: While simpler models like Logistic Regression performed reasonably well, the Random Forest's ability to capture non-linear relationships provided superior results.\n",
    "\n",
    "4. **Balance of Metrics**: In healthcare applications, considering multiple performance metrics (not just accuracy) is crucial. The F1 score proved valuable as it balances precision and recall.\n",
    "\n",
    "5. **Hyperparameter Sensitivity**: Random Forest performance improved with tuning, demonstrating the importance of optimization even for relatively robust algorithms.\n",
    "\n",
    "6. **Interpretability Trade-offs**: While Random Forest performed well, its \"black box\" nature presents challenges for explaining predictions to medical professionals compared to simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6526a",
   "metadata": {},
   "source": [
    "## 14. Potential Usefulness to Society or Healthcare Industry\n",
    "\n",
    "This project has several potential applications in healthcare:\n",
    "\n",
    "1. **Screening Tool**: The model can be implemented as a preliminary screening tool to identify high-risk individuals who may benefit from more comprehensive testing.\n",
    "\n",
    "2. **Resource Allocation**: Healthcare providers can use risk predictions to prioritize limited resources for diabetes prevention and management.\n",
    "\n",
    "3. **Personalized Risk Assessment**: The feature importance analysis can help develop personalized risk profiles based on individual health metrics.\n",
    "\n",
    "4. **Public Health Campaigns**: Insights about key risk factors can inform targeted public health education and prevention campaigns.\n",
    "\n",
    "5. **Clinical Decision Support**: The model can be integrated into electronic health record systems to provide real-time risk assessments during patient consultations.\n",
    "\n",
    "6. **Research Guidance**: The identified relationships between variables can guide further medical research into diabetes risk factors.\n",
    "\n",
    "7. **Remote Healthcare**: The model can be deployed in telehealth applications where in-person diagnostic testing may be limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5f7b9",
   "metadata": {},
   "source": [
    "## 15. Future Extensions of the Project\n",
    "\n",
    "This project could be extended in several valuable directions:\n",
    "\n",
    "1. **Additional Features**: Incorporating more health indicators like family history details, lifestyle factors (diet, exercise), and socioeconomic factors.\n",
    "\n",
    "2. **Temporal Analysis**: If longitudinal data becomes available, developing models that track how risk changes over time based on changing health metrics.\n",
    "\n",
    "3. **Risk Stratification**: Evolving from binary classification to multi-class classification that identifies different levels of risk.\n",
    "\n",
    "4. **Deep Learning Approaches**: Experimenting with neural networks to capture more complex patterns, especially if the dataset expands.\n",
    "\n",
    "5. **Explainable AI Methods**: Implementing techniques like SHAP (SHapley Additive exPlanations) values to provide more interpretable predictions.\n",
    "\n",
    "6. **Mobile Application**: Developing a user-friendly mobile app that allows individuals to assess their diabetes risk and receive personalized prevention recommendations.\n",
    "\n",
    "7. **Integration with IoT Devices**: Connecting the model with wearable health monitors to provide continuous risk assessment based on real-time health data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5adcf84",
   "metadata": {},
   "source": [
    "## 16. Conclusion Based on Findings\n",
    "\n",
    "The diabetes classification project successfully developed a machine learning model capable of predicting diabetes risk with high accuracy using the Pima Indians dataset. The tuned Random Forest classifier achieved excellent performance metrics, demonstrating its potential as a valuable tool for healthcare professionals.\n",
    "\n",
    "Key findings include:\n",
    "\n",
    "1. **Prediction Performance**: The model achieved strong performance metrics, with particular strength in correctly identifying high-risk patients.\n",
    "\n",
    "2. **Critical Indicators**: Plasma glucose concentration emerged as the single most important predictor, followed by BMI and age, confirming their clinical significance in diabetes risk assessment.\n",
    "\n",
    "3. **Data Preprocessing Impact**: Proper handling of missing values significantly improved model performance, highlighting the importance of domain knowledge in data preparation.\n",
    "\n",
    "4. **Model Generalization**: The tuned Random Forest showed good generalization ability with only a small gap between training and test performance, suggesting it would be reliable on new patient data.\n",
    "\n",
    "This project demonstrates how machine learning can effectively support healthcare decision-making by providing accurate risk assessments based on readily available medical measurements. While not a replacement for clinical judgment, such tools can enhance early detection efforts and help prioritize preventive interventions for at-risk individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8547f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of model usage for a new patient\n",
    "def predict_diabetes(pregnancies, glucose, blood_pressure, skin_thickness, insulin, bmi, diabetes_pedigree, age):\n",
    "    \"\"\"\n",
    "    Make a diabetes prediction for a new patient.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pregnancies: int\n",
    "        Number of pregnancies\n",
    "    glucose: float\n",
    "        Plasma glucose concentration\n",
    "    blood_pressure: float\n",
    "        Diastolic blood pressure (mm Hg)\n",
    "    skin_thickness: float\n",
    "        Triceps skinfold thickness (mm)\n",
    "    insulin: float\n",
    "        2-Hour serum insulin (mu U/ml)\n",
    "    bmi: float\n",
    "        Body mass index (weight in kg/(height in m)Â²)\n",
    "    diabetes_pedigree: float\n",
    "        Diabetes pedigree function\n",
    "    age: int\n",
    "        Age in years\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction: int\n",
    "        0: Non-diabetic, 1: Diabetic\n",
    "    probability: float\n",
    "        Probability of being diabetic\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with the patient data\n",
    "    patient_data = pd.DataFrame({\n",
    "        'Pregnancies': [pregnancies],\n",
    "        'Glucose': [glucose],\n",
    "        'BloodPressure': [blood_pressure],\n",
    "        'SkinThickness': [skin_thickness],\n",
    "        'Insulin': [insulin],\n",
    "        'BMI': [bmi],\n",
    "        'DiabetesPedigreeFunction': [diabetes_pedigree],\n",
    "        'Age': [age]\n",
    "    })\n",
    "    \n",
    "    # Handle missing values (zeros) if any\n",
    "    for feature in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:\n",
    "        if patient_data[feature].iloc[0] == 0:\n",
    "            patient_data[feature] = processed_data[feature].median()\n",
    "    \n",
    "    # Scale the features\n",
    "    patient_data_scaled = scaler.transform(patient_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_tuned_model.predict(patient_data_scaled)[0]\n",
    "    probability = best_tuned_model.predict_proba(patient_data_scaled)[0][1]  # Probability of being diabetic\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "sample_patient = {\n",
    "    'pregnancies': 6,\n",
    "    'glucose': 148,\n",
    "    'blood_pressure': 72,\n",
    "    'skin_thickness': 35,\n",
    "    'insulin': 0,\n",
    "    'bmi': 33.6,\n",
    "    'diabetes_pedigree': 0.627,\n",
    "    'age': 50\n",
    "}\n",
    "\n",
    "prediction, probability = predict_diabetes(**sample_patient)\n",
    "\n",
    "print(\"Sample Patient Data:\")\n",
    "for key, value in sample_patient.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "print(f\"\\nPrediction: {'Diabetic' if prediction == 1 else 'Non-Diabetic'}\")\n",
    "print(f\"Probability of Diabetes: {probability:.2%}\")\n",
    "\n",
    "# Visualize the prediction probability\n",
    "plt.figure(figsize=(10, 6))\n",
    "probabilities = [1-probability, probability]\n",
    "classes = ['Non-Diabetic', 'Diabetic']\n",
    "colors = ['skyblue', 'coral']\n",
    "plt.bar(classes, probabilities, color=colors)\n",
    "plt.title('Prediction Probability')\n",
    "plt.ylabel('Probability')\n",
    "plt.ylim(0, 1)\n",
    "for i, prob in enumerate(probabilities):\n",
    "    plt.text(i, prob + 0.02, f'{prob:.4f}', ha='center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
